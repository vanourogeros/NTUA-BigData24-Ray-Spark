import sys
from typing import List

import numpy as np
from pyspark.sql import SparkSession
import os
import time
from sparkmeasure import TaskMetrics, StageMetrics
from pyspark.sql.functions import col, sqrt, length
from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans
os.environ['PYSPARK_PYTHON'] = sys.executable
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

def parseVector(line: str) -> np.ndarray:
    try:
        return np.array([float(x) for x in line.split(',')[:3]])
    except:
        return


def closestPoint(p: np.ndarray, centers: List[np.ndarray]) -> int:
    bestIndex = 0
    closest = float("+inf")
    for i in range(len(centers)):
        tempDist = np.sum((p - centers[i]) ** 2)
        if tempDist < closest:
            closest = tempDist
            bestIndex = i
    return bestIndex


if __name__ == "__main__":

    #if len(sys.argv) != 4:
     #   print("Usage: kmeans <file> <k> <convergeDist>", file=sys.stderr)
      #  sys.exit(-1)

    sparky = SparkSession \
    .builder \
    .appName("load_data") \
    .master("yarn") \
    .config("spark.executor.instances", "3") \
    .config("spark.executor.cores", "4") \
    .config("spark.jars.packages", "ch.cern.sparkmeasure:spark-measure_2.12:0.23") \
    .getOrCreate()

    sc = sparky.sparkContext

    stagemetrics = StageMetrics(sparky)
    #stagemetrics.begin()

    data = sparky.read.format("csv") \
           .option("header", "true") \
           .option("inferSchema", "true") \
           .load([f"hdfs://okeanos-master:54310/data/large/dataset{i}.csv" for i in range(1,4)])
    data = data.withColumn(
    "new_feature",
    sqrt(col("feature_1") ** 2 + col("feature_2") ** 2)
    ).filter(length(col("word")) > col("new_feature"))

    data = data.select(data.feature_1, data.feature_2, data.feature_3, data.new_feature)

    assembler = VectorAssembler(inputCols=["feature_1", "feature_2", "feature_3", "new_feature"],
                             outputCol="features")
    pipeline = Pipeline(stages=[assembler])
    scalerModel = pipeline.fit(data)
    data = scalerModel.transform(data).select(['features'])


    #lines = sparky.read.text('hdfs://okeanos-master:54310'+sys.argv[1])
    #data = lines.map(parseVector).cache()
    #data = sc.parallelize(data.take(int(sys.argv[2])))
    #data = data.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])
    #print(data.take(10))
    stagemetrics.begin()
    K = 2
    convergeDist = 0
    kPoints = data.take(K)
    tempDist = 1.0

    # Trains a k-means model.
    kmeans = KMeans().setK(2).setSeed(1).setMaxIter(10).setTol(0)
    model = kmeans.fit(data)

    # Make predictions
    predictions = model.transform(data)

    # Evaluate clustering by computing Silhouette score
    #evaluator = ClusteringEvaluator()

    #silhouette = evaluator.evaluate(predictions)
    #print("Silhouette with squared euclidean distance = " + str(silhouette))

    # Shows the result.
    centers = model.clusterCenters()
    print("Cluster Centers: ")
    for center in centers:
        print(center)

    stagemetrics.end()
    print(stagemetrics.aggregate_stagemetrics())

    # memory report needs a bit of time to run...
    patience = 20
    while patience > 0:
        try:
            stagemetrics.print_memory_report()
            patience = -1
        except:
            print("memory report not ready")
            time.sleep(1)
            patience -= 1
    if patience == 0:
        print("memory report never ready :(")
    print("Final centers: " + str(kPoints))

    sparky.stop()
